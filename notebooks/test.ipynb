{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2624f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from chromadb.utils import embedding_functions\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=openai_key, model_name=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "# chroma client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_persistent_storage\")\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,embedding_function=openai_ef\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "def query_documents(question, n_results=5):\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=openai_key, model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name, embedding_function=openai_ef\n",
    "    )\n",
    "    print(f\"üîç Querying collection: {collection.name}\")\n",
    "\n",
    "    results = collection.query(query_texts=question, n_results=n_results)\n",
    "    relevant_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "\n",
    "    citations = []\n",
    "    for idx, _ in enumerate(results[\"documents\"][0]):\n",
    "        doc_id = results[\"ids\"][0][idx]\n",
    "        citations.append(doc_id)\n",
    "\n",
    "    return relevant_chunks, citations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_advanced_prompt(question, context, citations):\n",
    "    # Format citations into the context\n",
    "    formatted_citations = \"\\n\".join(\n",
    "        f\"[Source {citation.replace('_', ' ')}]\" for citation in citations\n",
    "    )\n",
    "    \n",
    "    return f\"\"\"\n",
    "    # Retrieval-Augmented Generation (RAG) Prompt\n",
    "\n",
    "    ## Context Specification\n",
    "    - Question Domain: Precise Information Retrieval\n",
    "    - Retrieval Methodology: Semantic Search\n",
    "    - Citation Requirement: Mandatory\n",
    "\n",
    "    ## Question\n",
    "    {question}\n",
    "\n",
    "    ## Available Knowledge Sources\n",
    "    {context}\n",
    "\n",
    "    ## Source Citations\n",
    "    {formatted_citations}\n",
    "\n",
    "    ## Response Guidelines\n",
    "    1. Answer ONLY using provided sources\n",
    "    2. Cite sources explicitly for each claim\n",
    "    3. Cite in format: [Source #]\n",
    "    4. If information is insufficient, state limitations\n",
    "    5. Maintain academic rigor in response\n",
    "    6. Keep your answer small and precise\n",
    "    7. Focus only on important pages\n",
    "\n",
    "    ## Citation Instruction\n",
    "    - Directly attribute information to sources\n",
    "    - Use [Source #] immediately after relevant information\n",
    "    - Highlight source relevance and confidence\n",
    "    \"\"\"\n",
    "    \n",
    "def format_chunks(relevant_chunks, citations):\n",
    "    formatted_chunks = []\n",
    "    for i in range(len(relevant_chunks)):\n",
    "        try:\n",
    "            citation = citations[i] if citations and i < len(citations) else f\"Unknown Source {i+1}\"\n",
    "            chunk = relevant_chunks[i] if relevant_chunks and i < len(relevant_chunks) else \"No content available\"\n",
    "            clean_citation = str(citation).split('.')[0].strip()\n",
    "            formatted_chunk = f\"[Source {clean_citation}]:\\n{chunk}\"\n",
    "            formatted_chunks.append(formatted_chunk)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Fallback for any unexpected errors\n",
    "            formatted_chunks.append(f\"[Source Error]: Unable to format source {i+1}\")\n",
    "    \n",
    "    return formatted_chunks\n",
    "\n",
    "def generate_response(question, formatted_chucks, citations):\n",
    "\n",
    "    context = \"\\n\\n\".join(formatted_chucks)\n",
    "    prompt = construct_advanced_prompt(question, context, citations)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def chunks_used_by_ai(df,pages):\n",
    "    ai_pages = [\"page_\"+str(page) for page in pages]\n",
    "    filtered_df = df[df['Pages'].isin(ai_pages)]\n",
    "    grouped_df = filtered_df.groupby('Pages')['Chunks'].apply(list).reset_index()\n",
    "    return grouped_df\n",
    "    \n",
    "    \n",
    "def retrieve_and_generate(question):\n",
    "    chunks,citations = query_documents(question)\n",
    "\n",
    "    extracted_citations = [citation.split('.')[0] for citation in citations]\n",
    "    formatted_chunks = format_chunks(chunks, citations)\n",
    "    ai_response = generate_response(question, formatted_chunks, extracted_citations).replace(\"_\",\" \")\n",
    "    # print(ai_response)\n",
    "    \n",
    "    # print(chunks)\n",
    "    matches = re.findall(r'age (\\d+)\\]', ai_response)\n",
    "\n",
    "    # Convert matches to a sorted list of unique page numbers\n",
    "    pages = sorted(set(map(int, matches)))\n",
    "    # print(chunks)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Chunks': chunks,\n",
    "        'Pages': extracted_citations\n",
    "    })\n",
    "\n",
    "    ai_chunks = chunks_used_by_ai(df,pages)\n",
    "    \n",
    "    return ai_response, pages, ai_chunks\n",
    "\n",
    "\n",
    "def compute_precision_recall(retrieved_ids, ground_truth_ids):\n",
    "    retrieved_set = set(retrieved_ids)\n",
    "    ground_truth_set = set(ground_truth_ids)\n",
    "    true_positives = retrieved_set & ground_truth_set\n",
    "\n",
    "    precision = len(true_positives) / len(retrieved_set) if retrieved_set else 0\n",
    "    recall = len(true_positives) / len(ground_truth_set) if ground_truth_set else 0\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def evaluate_question(question, ground_truth_ids):\n",
    "    chunks, retrieved_ids = query_documents(question)\n",
    "    precision, recall = compute_precision_recall(retrieved_ids, ground_truth_ids)\n",
    "\n",
    "    print(f\"üß† Question: {question}\")\n",
    "    print(f\"üìå Retrieved: {retrieved_ids}\")\n",
    "    print(f\"# Retrieved chunks: {chunks}\")\n",
    "    print(f\"‚úÖ Ground Truth: {ground_truth_ids}\")\n",
    "    print(f\"üìä Precision: {precision:.2f}\")\n",
    "    print(f\"üìä Recall: {recall:.2f}\")\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e9bb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def load_chunks_from_sqlite(db_path=\"../application.db\"):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM chunks\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0598424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>page_5.md_chunk1</td>\n",
       "      <td>Eventually, structural embedding Pg of node vg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>page_5.md_chunk2</td>\n",
       "      <td>tive responses to the preceding questions\\ncon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>page_5.md_chunk3</td>\n",
       "      <td>ully\\navailable textual information while S?#\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>page_5.md_chunk4</td>\n",
       "      <td>nformation leakage. For the\\nlarger Arxiv netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>page_5.md_chunk5</td>\n",
       "      <td>order when the review was generated.\\n\\nProdu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>page_2.md_chunk2</td>\n",
       "      <td>answer the second question, we analyze\\nthe co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>page_2.md_chunk3</td>\n",
       "      <td>nerated.\\n\\ne Comprehensive Empirical Analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>page_2.md_chunk4</td>\n",
       "      <td>h\\nemail through the sentence-transformer [45]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>page_2.md_chunk5</td>\n",
       "      <td>he\\nsame roles, indicating their local subgrap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>page_2.md_chunk6</td>\n",
       "      <td>feeding\\nits abstract through sentence-transfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                               text\n",
       "0   page_5.md_chunk1  Eventually, structural embedding Pg of node vg...\n",
       "1   page_5.md_chunk2  tive responses to the preceding questions\\ncon...\n",
       "2   page_5.md_chunk3  ully\\navailable textual information while S?#\"...\n",
       "3   page_5.md_chunk4  nformation leakage. For the\\nlarger Arxiv netw...\n",
       "4   page_5.md_chunk5   order when the review was generated.\\n\\nProdu...\n",
       "..               ...                                                ...\n",
       "66  page_2.md_chunk2  answer the second question, we analyze\\nthe co...\n",
       "67  page_2.md_chunk3  nerated.\\n\\ne Comprehensive Empirical Analysis...\n",
       "68  page_2.md_chunk4  h\\nemail through the sentence-transformer [45]...\n",
       "69  page_2.md_chunk5  he\\nsame roles, indicating their local subgrap...\n",
       "70  page_2.md_chunk6  feeding\\nits abstract through sentence-transfo...\n",
       "\n",
       "[71 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_chunks_from_sqlite()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c9e2257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Querying collection: document_qa_collection\n",
      "üß† Question: tell me about topo rag?\n",
      "üìå Retrieved: ['page_8.md_chunk4', 'page_8.md_chunk6', 'page_9.md_chunk6', 'page_5.md_chunk2', 'page_7.md_chunk1']\n",
      "# Retrieved chunks: ['eature rates from 0.1 to 0.9. We can observe that\\nTopoRAG consistently outperforms other strategies in both node\\nclassification and link prediction across all rates of missing features.\\nThis underscores the benefits of incorporating additional context\\nin handling missing feature issues on graphs.\\n\\n6.4 Feature Imputation with TopoRAG\\n\\nMany machine learning models assume a fully observed feature\\nmatrix. However, in practice, each feature is only observed for a\\nsubset of nodes due to constraints like privacy concerns or limited\\nresources for data annotation [67]. In all these scenarios, the missing\\nfeature issues could catastrophically compromise the capability of\\nmachine learning models [47], which motivates many previous\\nworks developing solutions to handling missing feature issue [68].\\n\\nSince our proposed TopoRAG can naturally generate node fea-\\ntures in graph-based datasets, in this section, we evaluate its effec-\\ntiveness in handling missing features by comparing its performance\\n\\n7 R', 'ethods by equipping it with topology awareness. Different from\\nKG-based RAG where topology information is incorporated by re-\\ntrieving triples from subgraphs around entities mentioned in the\\nquestion [61], we explicitly consider proximity and role-based topo-\\nlogical relations in guiding the retrieval, the related works of which\\nare reviewed next.\\n\\n', 'ains. This\\nfurther justifies why TopoRAG achieves almost consistently higher\\nperformance than other baselines on all these datasets.\\n\\n', 'tive responses to the preceding questions\\nconfirm two key insights. Firstly by the answer to Q;, LLMs sig-\\nnificantly benefit from incorporating additional texts during text\\ngeneration. The closer the resemblance of these additional texts to\\nthe generated one, the greater the enhancement is in the output. Sec-\\nondly, by the answer to Qz, there is a positive correlation between\\ntextual and topological similarity. Drawing from these findings, we\\npropose a novel framework using topological similarity to guide\\nthe retrieval of additional texts, thereby augmenting the quality\\nof the generated text, i.e., Topology-aware Retrieval-Augmented\\nGeneration (Topo-RAG), which is introduced next.\\n\\n5 Framework of Topo-RAG\\n\\nFollowing Eq. (1), the retriever works by retrieving top-K nodes\\naccording to their topological similarity to the target text as follows:\\n\\ng- ‚Äî ~Retriever;yv eFull py _ KK 4 Topo;,. .. \\\\ (7\\n\\n6 Experiments\\n\\nTable 1: Statistics of Datasets. SF\"! denote nodes with fully\\navailable textu', 'Table 2: Performance comparison of TopoRAG with baselines. The best results are in bold. BLEU is BLEU-4, ROUGE is ROUGE-L.\\nOur TopoRAG almost achieves the best performance across all baselines on all datasets. \"Average\" is computed by averaging\\n\\neach metric across 9 datasets. \"Boost\" is computed by the relative performance gain from the second-to-best \"Text\" to the best\\n\"TopoRAG\".\\n\\nTable 3: Task-oriented Evaluation by comparing the node\\nclassification and link prediction performance of different\\nbaselines. The best results are in bold. NC - Node Classifica-\\ntion; LP - Link Prediction.\\n\\nnode classification and link prediction on the Cora and Pubmed\\n\\n6.2.2 Task-oriented Evaluation. In addition to conventional met-\\nrics, we also utilize task-oriented metrics to assess the quality of\\nthe generated texts. Specifically, we evaluate the performance of\\nnode classification and link prediction using the generated texts\\nfrom different baselines. As shown in Table 3, TopoRAG consistently\\nachieves ']\n",
      "‚úÖ Ground Truth: ['', '', '']\n",
      "üìä Precision: 0.00\n",
      "üìä Recall: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "question = \"tell me about topo rag?\"\n",
    "ground_truth = [\"\", \"\", \"\"]\n",
    "\n",
    "evaluate_question(question, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
